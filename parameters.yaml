# ---------------------------- Data processing ------------------------------- #
# Results are done by cell line and will go in the appropriate directory.
cell_line: "hela"


# ---------------------------- Execution ------------------------------------- #

#### Model

# If true, you will use only artificial data. Useful for demonstration.
use_artificial_data: False

# If you have already trained a model, set this to True (models are saved automatically when trained)
load_saved_model: False

# If reloading a known model, you may want to set this to false and skip the diangnosis steps (q-score, etc.)
perform_model_diagnosis: True

#### Data processing

# If False, will not process the full real data, which can be long
# You should calibrate with Q-score before processing the full data (see readme) !
process_full_real_data: True

# The below will perform diagnosis on real data : average score per crm, alone+both plots, that kind of stuff
# You should likely leave it at True
perform_real_data_diagnosis: True


# ---------------------------- Autoencoder model ----------------------------- #

# Below are the key parameters for info budget

## Architecture
nn_kernel_nb: 32
nn_deep_dim: 256
## Training
nn_optimizer_learning_rate: 0.0001  # NOTE Default for Adam is 0.001



# ------ You likely won't need to modify those below
pad_to: 3200 # Must be equal to the "pad_to" parameter in the Makefile !
crumb: 0.1
squish_factor: 10

## Other training paramters Training
nn_batch_size: 48
nn_optimizer: "adam"

# Long to train. Keep these both small. Usually long training not needed and will stop before.
nn_batches_per_epoch : 48
nn_number_of_epochs : 64 # NOTE Old was 24, but I have found better to use more and let the EARLY STOPPING take care of it
nn_fit_verbosity: 1

# Early stopping parameters
nn_early_stop_min_delta: 0.00025
nn_early_stop_patience: 5
nn_early_stop_loss_absolute: 0 # Will stop training once this threshold of loss is reached
# TODO : USE IT IN THE CODE FOR NOW IT DOES NOTHING


## Other architecture parameters Architecture
#autoencoder_parameters:
nn_kernel_width_in_basepairs: 20 # Should be kept small, use squishing instead
nn_reg_coef_filter: 0.0025
nn_pooling_factor: 2


# ---------------------------- Diagnostic ------------------------------------ #


###### Artificial data parameters
artificial_nb_datasets: 8
artificial_nb_tfs: 8
artificial_ones_only: True
artificial_watermark_prob: 0.75
artificial_tfgroup_split: 0.666
artificial_overlapping_groups: False

# Core parameter : used all across the program, including in training !
random_seed: 666 # Replace with 42 :)

# Distribution of scores depending on TF presence pairs to visualize
# Only used in real data, not artificial

# FOR JURKAT
# tf_pairs:
#   - ["erg","gabpa"]
#   - ["ctcf","gabpa"]
#   - ["ctcf","runx1"]

#FOR HELA
tf_pairs:
  - ["ell2", "aff4"]
  - ["e2f1", "phf8"]
  - ["aff4", "znf143"]
  - ["brd4", "rcor1"]
  - ["znf143", "rcor1"]
  - ["ell2", "phf8"]
  - ["ell2", "brd4"]]

# FOR K562:
# tf_pairs:
#   - ["ctcf","rad21"]
#   - ["fos","jun"]
#   - ["irf1","atf1"]
#   - ["ctcf","jun"]
#   - ["gata1","gata2"]
