# ---------------------------- Data processing ------------------------------- #
# Results are done by cell line and will go in the appropriate directory.
cell_line: "hela"
# Change this parameter to process a different cell line.
# Will be ignored when using artificial data.

# Path to the CRM file. Should have the same value than in the Makefile.
CRM_FILE: "./data/input_raw/remap2018_crm_macs2_hg38_v1_2_selection.bed"

# ---------------------------- Execution ------------------------------------- #

# If you have already trained a model, set this to True (models are saved automatically when trained)
load_saved_model: False

# If False, will not process the full real data, which can be long
# You should calibrate with Q-score before processing the full data (see readme) !
process_full_real_data: True


#### Diagnosis parameters
# Most users will not need to modify this

# If reloading a known model, you may want to set this to False and skip the diagnosis steps (Q-score, etc.)
perform_model_diagnosis: True

# The model will perform diagnosis on result data, after processing : average score per crm, alone+both plots, etc.
# You should likely leave it at True.
perform_processed_data_diagnosis: True

# If True, use only artificial data instead to perform a proof-of-concept.
use_artificial_data: True

#### Others
random_seed: 1234 # Used all across the program, including in training

disable_tensorflow_warnings: True

# ---------------------------- Autoencoder model ----------------------------- #

# ------ Those are the key parameters for the information budget

## Architecture
nn_kernel_nb: 24
nn_deep_dim: 32
## Training
nn_optimizer_learning_rate: 0.0001 # NOTE Default for Adam is 0.001 (!)


# ------ You likely won't need to modify the parameters below

# Weighting
# Should be a list containing as many elements as the relevant dimension. Gives their respective weights (sorted alphabetically)
# eg. tf_weights = [1,1,2,2]
# if `null`, default weight of 1 is used.
tf_weights: null
datasets_weights: null

pad_to: 3200 # WARNING Must be equal to the "pad_to" parameter in the Makefile !
crumb: 0.1
squish_factor: 10

## Other training paramters
nn_batch_size: 48
nn_optimizer: "adam"

# Long to train. Keep these both small. Usually long training not needed and will stop before.
nn_batches_per_epoch : 48
nn_number_of_epochs : 64 # Better to use more and use early stopping
nn_fit_verbosity: 1

# Early stopping parameters
nn_early_stop_min_delta: 0.00025  # Loss delta below this are not considered improvements
nn_early_stop_patience: 5
nn_early_stop_loss_absolute: 0 # Will stop training once this threshold of loss is reached
# NOTE early stopping before the loss has reached a plateau can be very useful !

## Other architecture parameters
nn_kernel_width_in_basepairs: 20 # Should be kept small, use squishing instead
nn_reg_coef_filter: 0.0025
nn_pooling_factor: 2


# How many workers to use when multithreading file production. At least 1.
nb_workers_produce_file: 4 
# NOTE Higher number of processes is not always faster, and will increase RAM footprint which can be problematic for very large models/dimensions

# ---------------------------- Diagnostic ------------------------------------ #

# The mean score by TF will be normalized (X-mean/std) to be around this value (use one between 0 and 1000)
# So after corr group normalization, we have a larger part of the score palette dedicated to "worse-than-typical" and not "better-than-typical"
tf_normalization_center_around: 750


# Several diagnostic steps are done only on a randomized subset of CRMs ; the subset is generated one.
# This parameter betermines how many batches are drawn to geenrate the subset
nb_batches_generator_for_diag_list_of_many_crms: 200
# This is criticial, because it can take a very long time for high-dimensional data


## Distribution of scores depending on TF presence.
# Gives the [A, B] pairs to visualize whether the presence of one affects the score of the other
# Only used in real data, not artificial
tf_pairs:
  - ["tf_2", "tf_3"]
  - ["tf_2", "tf_6"]

## Correlation group estimation
# By looking at what kinds of phantoms are added, estimate the correlation groups learned by the model for the following sources
# Each line should contain a source, which is a {dataset, TF} pair given in that order below
estimate_corr_group_for:
  - ['dat_0','tf_0']
  - ['dat_5','tf_2']
  - ['dat_5','tf_6']

# ------  You likely won't need to modify the parameters below

# Get some examples of CRMs' representations and rebuilding by the models. Number of batches to be given as example.
example_nb_batches: 1

### Artificial data parameters

artificial_nb_datasets: 8
artificial_nb_tfs: 16
artificial_ones_only: True
artificial_watermark_prob: 0 # Usually 0.75, lower for higher dimensions


artificial_tfgroup_split: 0.5 # Odds of selecting first group instead of second group. 0.666 was used to demonstrate group rebuilt value depends on completeness, not abundance

## Debug overrides
artificial_overlapping_groups: False  # If True, the groups will be {AB,A} instead of {A,B}
# NOTE The default values for the two parameters below are `null`, not 0 or None or False.
artificial_this_many_groups_of_4_tfs: null # Split all TFs in this many groups of size 4 TFs (don't use the rest of the tfs, noise only) and chosse uniformly randomly among them. Override previous two parameters
artificial_split_tfs_into_this_many_groups: 8 # Instead of 2 correlation groups, splints the artificiail_nb_tfs into this many groups, picking one uniromly randoly each time. Overrides the three previous parameters