# ---------------------------- Data processing ------------------------------- #
# Results are done by cell line and will go in the appropriate directory.
cell_line: "hela"
# Change this parameter to process a different cell line.

# Path to the CRM file. Should have the same value than in the Makefile.
CRM_FILE: "./data/input_raw/remap2018_crm_macs2_hg38_v1_2_selection.bed"

# ---------------------------- Execution ------------------------------------- #



#### Model

# If true, you will use only artificial data.
# Used to demonstrate that the model can learn correlation groups.
use_artificial_data: False

# If you have already trained a model, set this to True (models are saved automatically when trained)
load_saved_model: False

# If reloading a known model, you may want to set this to False and skip the diagnosis steps (Q-score, etc.)
perform_model_diagnosis: True

#### Data processing

# If False, will not process the full real data, which can be long
# You should calibrate with Q-score before processing the full data (see readme) !
process_full_real_data: True

# The below will perform diagnosis on real data : average score per crm, alone+both plots, that kind of stuff
# You should likely leave it at True
perform_real_data_diagnosis: True


#### Others

# Used all across the program, including in training !
random_seed: 1234

disable_tensorflow_warnings: True

# ---------------------------- Autoencoder model ----------------------------- #

# ------ NOTE Below are the key parameters for info budget

## Architecture
nn_kernel_nb: 32
nn_deep_dim: 256
## Training
nn_optimizer_learning_rate: 0.0001  # NOTE Default for Adam is 0.001 (!)



# ------ NOTE You likely won't need to modify the parameters below
pad_to: 3200 # WARNING Must be equal to the "pad_to" parameter in the Makefile !
crumb: 0.1
squish_factor: 10

## Other training paramters
nn_batch_size: 48
nn_optimizer: "adam"

# Long to train. Keep these both small. Usually long training not needed and will stop before.
nn_batches_per_epoch : 48
nn_number_of_epochs : 64 # NOTE Old was 24, but I have found better to use more and let the EARLY STOPPING take care of it
nn_fit_verbosity: 1

# Early stopping parameters
nn_early_stop_min_delta: 0.00025  # Loss delta below this are not considered improvements
nn_early_stop_patience: 5
nn_early_stop_loss_absolute: 0 # Will stop training once this threshold of loss is reached


## Other architecture parameters
nn_kernel_width_in_basepairs: 20 # Should be kept small, use squishing instead
nn_reg_coef_filter: 0.0025
nn_pooling_factor: 2

# Weighting
# Should be a list containing as many elements as the relevant dimension. Gives their respective weights (sorted alphabetically)
# eg. tf_weights = [1,1,2,2]
# if `null`, default weight of 1 is used.
tf_weights: null
datasets_weights: null

# How many workers to use when multithreading file production. At least 1.
# For very large dimensions/models, this may use a lot of RAM.
nb_workers_produce_file: 7


# ---------------------------- Diagnostic ------------------------------------ #


###### Artificial data parameters
artificial_tfgroup_split: 0.666
artificial_overlapping_groups: False

artificial_nb_datasets: 8
artificial_nb_tfs: 8
artificial_ones_only: True
artificial_watermark_prob: 0.75


# Several diagnostic steps are done only on a randomized subset of CRMs ; the subset is generated one.
# This parameter betermines how many batches are drawn to geenrate the subset
nb_batches_generator_for_diag_list_of_many_crms: 200
# This is criticial, because it can take a very long time for high-dimensional data

# Get some examples of CRMs' representations and rebuilding by the models. Number of batches to be given as example.
example_nb_batches: 1



# The mean score by TF will be normalized (X-mean/std) to be around this value (use one between 0 and 1000)
# So after corr group normalization, we have a larger part of the score palette dedicated to "worse-than-typical" and not "better-than-typical"
tf_normalization_center_around: 750


## Distribution of scores depending on TF presence.
# Gives the [A, B] pairs to visualize whether the presence of one affects the score of the other
# Only used in real data, not artificial

# FOR JURKAT
# tf_pairs:
#   - ["erg","gabpa"]
#   - ["ctcf","gabpa"]
#   - ["ctcf","runx1"]

# FOR HELA
tf_pairs:
  - ["ell2", "aff4"]
  - ["e2f1", "phf8"]
  - ["aff4", "znf143"]
  - ["brd4", "rcor1"]
  - ["znf143", "rcor1"]
  - ["ell2", "phf8"]
  - ["ell2", "brd4"]
  - ["sfmbt1", "rcor1"]
  - ["sfmbt1", "brd4"]
  - ["brd4", "phf8"]

# FOR K562:
# tf_pairs:
#   - ["ctcf","rad21"]
#   - ["fos","jun"]
#   - ["irf1","atf1"]
#   - ["ctcf","jun"]
#   - ["gata1","gata2"]


## Correlation group estimation
# By looking at what kinds of phantoms are added, estimate the correlation groups learned by the model for the following sources
# Each line should contain a source, which is a {dataset, TF} pair given in that order below

# FOR HELA
estimate_corr_group_for:
    - ['GSE40632','aff4']
    - ['GSE45441','sfmbt1']
    - ['GSE22478','phf8']
    - ['GSE51633','brd4']
